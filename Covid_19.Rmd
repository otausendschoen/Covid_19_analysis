---
title: "Covid_19"
author: "Gerardo Goar, Oliver Tausendschön"
date: "2024-11-24"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Abstract



# Introduction


# Setup
```{r}
library(dplyr)
library(glmnet)
library(rstanarm) # Easy estimation of standard models with Bayesian methods 
#library(bayestestR) # Functions to analyze posterior distributions generated by rstranarm
library(mombf) # Bayesian model selection and Bayesian model averaging
library(HDCI)
library(boot)
library(ggplot2)

# Load necessary libraries
library(tidyverse)    # For data manipulation
library(cluster)      # For Gap Statistic
library(factoextra)   # For clustering visualization
#install.packages("patchwork")
library(patchwork)    # For combining plots
# Load necessary libraries
library(mclust)       # For Gaussian Mixture Models

```


```{r}
data<-read.csv("/home/oliver/Documents/Statisticcal_Modelling_Project/Data/covid_data.csv")
#source("routines_seminar1.R")

```

```{r}
set.seed(123)
```

# Preprocessing

```{r}
head(data)
print(sum(is.na(data$total_cases)))
```
Drop Columns that have information about Covid.

```{r}
covid_19 <- data %>%
  select(
    -c(
      "new_cases", "new_cases_smoothed", "total_deaths", "new_deaths", "new_deaths_smoothed",
      "total_cases_per_million", "new_cases_per_million", "new_cases_smoothed_per_million",
      "total_deaths_per_million", "new_deaths_per_million", "new_deaths_smoothed_per_million",
      "reproduction_rate", "icu_patients", "icu_patients_per_million", "hosp_patients",
      "hosp_patients_per_million", "weekly_icu_admissions", "weekly_icu_admissions_per_million",
      "weekly_hosp_admissions", "weekly_hosp_admissions_per_million", "total_tests", "new_tests",
      "total_tests_per_thousand", "new_tests_per_thousand", "new_tests_smoothed",
      "new_tests_smoothed_per_thousand", "positive_rate", "tests_per_case", "tests_units",
      "total_vaccinations", "people_vaccinated", "people_fully_vaccinated", "total_boosters",
      "new_vaccinations", "new_vaccinations_smoothed", "total_vaccinations_per_hundred",
      "people_vaccinated_per_hundred", "people_fully_vaccinated_per_hundred",
      "total_boosters_per_hundred", "new_vaccinations_smoothed_per_million",
      "new_people_vaccinated_smoothed", "new_people_vaccinated_smoothed_per_hundred"
    ),
    -starts_with("excess")
  )
```



```{r}
# Filter dataset and select the first date when total_cases is at least 1
initial_cases <- covid_19 %>%
  filter(total_cases >= 1) %>%
  group_by(location) %>%
  arrange(date) %>%
  slice(1) %>%   # Keep only the first observation per country
  ungroup()
max_date <- covid_19 %>%
  group_by(location) %>%
  slice_max(total_cases, n = 1, with_ties = FALSE) %>% 
  ungroup()

# View the cleaned dataset
head(initial_cases)
```

```{r}
# Define the percentage threshold
threshold_pct <- 20  # Set percentage threshold, e.g., 20%
threshold <- ncol(initial_cases) * threshold_pct / 100  # Calculate the number of NAs corresponding to the percentage
cat("original amount of Na:", sum(is.na(initial_cases)))
# Step 1: Remove rows with more missing values than the threshold
initial_cases <- initial_cases %>%
  filter(apply(., 1, function(x) sum(is.na(x)) <= threshold))  # Use apply to check each row

# Step 2: Impute remaining NAs with the median of each column
initial_cases <- initial_cases %>%
  mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
cat("\nafter imputation amount of Na:", sum(is.na(initial_cases)))

# View the cleaned and imputed data
print(initial_cases)

```



```{r}
max_date <- covid_19 %>%
  filter(!is.na(total_cases)) %>% # Remove rows with NA in total_cases
  group_by(location) %>%
  slice_max(total_cases, n = 1, with_ties = FALSE) %>% 
  ungroup()

# View the resulting dataset
head(max_date)
print(sum(is.na(max_date$total_cases)))

df_with_nans <- max_date[is.na(max_date$total_cases), ]
```

Merging these dataframes:

```{r}

covid_19<-merge(initial_cases, max_date[, c("location", "total_cases")], by.x = "location", by.y = "location")

#covid_19 <- covid_19 %>%
#  select(-total_cases.x, -date)
# Create dummy variables manually
covid_19 <- covid_19 %>%
  mutate(
    continent_Africa = ifelse(continent == "Africa", 1, 0),
    continent_Asia = ifelse(continent == "Asia", 1, 0),
    continent_Europe = ifelse(continent == "Europe", 1, 0),
    continent_North_America = ifelse(continent == "North America", 1, 0),
    continent_Oceania = ifelse(continent == "Oceania", 1, 0),
    continent_South_America = ifelse(continent == "South America", 1, 0)
  ) %>%
  select(-continent, -total_cases.x, -date)  # Optionally, remove the original 'continent' column

# View the resulting dataframe
head(covid_19)
str(covid_19)
```



```{r}
# Assuming your dataset is named df

# Step 1: Select columns 7 to 21 for interaction terms
columns_to_exclude <- c("location", "iso_code", "total_cases.y", 
                        "continent_Africa", "continent_Asia", "continent_Europe", 
                        "continent_North_America", "continent_Oceania", "continent_South_America")#exclude categorical variables
covid_19_temp <- covid_19 %>%
  select(-one_of(columns_to_exclude))
#df_selected <- covid_data_model[, c(-1, -2, -19, -20, -21, -22,-23, -24, -25)]  # Select columns 7 to 21

# Step 2: Create pairwise interactions
covid_19_temp <- model.matrix(~ .^2, data = covid_19_temp)  # This generates pairwise interactions

# Step 3: Create three-way interactions
#df_three_way <- model.matrix(~ .^3, data = df_selected)  # This generates pairwise + three-way interactions

# Step 4: Combine the interactions with the original dataset (if needed)
#df_with_interactions <- cbind(df, df_pairwise[, -1], df_three_way[, -(1:ncol(df_selected))])
covid_19 <- cbind(covid_19[, columns_to_exclude], covid_19_temp[, -1])  # Remove the intercept column
# Select the columns you want to append from the original dataset
#columns_to_append <- covid_data_model[, c(1, 2, 19, 20, 21, 22, 23, 24, 25)]

# Combine them with df_with_interactions
#df_with_all_columns <- cbind(df_with_interactions, columns_to_append)
#df_with_all_columns<-df_with_all_columns[, -1]
```




```{r}
covid_19 <- covid_19[covid_19$location != "World", ]
covid_19_countries<-covid_19 #safe covid_19 with countries
covid_19 <- covid_19[, !names(covid_19) %in% c("location", "iso_code")]

covid_19 <- covid_19 %>%
  mutate(across(where(is.character), as.numeric))  # Convert character columns to numeric
```

## Scaling

```{r}
# Separate the target variable
y <- covid_19$total_cases

# Select predictors (all columns except the target)
x <- covid_19[, names(covid_19) != "total_cases.y"]
x_noscale<-x
# Standardize the predictors (mean 0, std. dev. 1)
x_to_scale <- x[, !(names(x) %in% columns_to_exclude)] #exclude categorical variables

# Scale the selected predictors (mean 0, std. dev. 1)
x <- scale(x_to_scale)

# Combine scaled predictors with the unscaled target
covid_19_scaled<-covid_19[, names(covid_19) %in% columns_to_exclude]
covid_19_scaled <- cbind(x, covid_19_scaled)
covid_19_scaled<- as.data.frame(covid_19_scaled)
x_scaled <- covid_19_scaled[, names(covid_19_scaled) != "total_cases.y"]

```


# Part 1


## Ordinary Least Squares

```{r}
fit.mle <- lm(covid_19_scaled$total_cases.y ~., data = covid_19_scaled)
b.mle <- coef(fit.mle)
summary(fit.mle)
```
It overfits!


Let's check the covariance matrix of full data:

```{r}
# Correlation between predictors and the target
cor_matrix <- cor(x, y)
# View summary of correlations
summary(cor_matrix)
```

We can also check specific entries. 
```{r}
#cor(covid_19)[23:40,20:40]

```

## L0 Penalty

CAUTION: RUNNING THIS CAN TAKE A LONG TIME
```{r, echo=FALSE, results="hide"}
#hide output as it is quite long
# Perform stepwise regression (both directions: forward and backward)
step_model <- step(fit.mle, direction = "both")# uncommented to save time

summary(step_model)

```


Let's see if we have less predictors
```{r}

cat("nonzeros in mle:", length(fit.mle$coefficients))

cat("\n")

cat("nonzeros in mle with stepwise method:", length(step_model$coefficients))

```


## Lasso Penalty

## Comparing Predictive Ability of LASSO with Different Lambda Criterias

To evaluate the predictive ability of the LASSO models with different methods for setting $\lambda$, we will split the dataset into training and testing subsets and compute performance metrics such as the **Mean Squared Prediction Error (MSPE)** and **R-squared**.

---

### Splitting the Data into Training and Testing Sets

```{r}
set.seed(123) # For reproducibility
train_indices <- sample(1:nrow(x), size = 0.8 * nrow(x)) # 80% for training
x_train <- x_noscale[train_indices, ]
y_train <- y[train_indices]
x_train_scaled <- x_scaled[train_indices, ]

x_test <- x_noscale[-train_indices, ]
x_test_scaled <- x_scaled[-train_indices, ]

y_test <- y[-train_indices]
```



We will now use the `glmnet` package that implements LASSO, Ridge and ElasticNet
penalizations for linear regression.

We use function `cv.glmnet` to select a $\lambda$ via 10-fold cross-validation.  `alpha` is by default set at 1, so we are fitting a LASSO regression. Of all the tried values for $\lambda$ `cv.glmnet` outputs two "best" ones.


When $p$ is large in comparison to $n$, MLE estimator variance can be very large due to estimating too many parameters. When $p$ is larger than $n$, OLS fails.

### Lasso CV

```{r}
#coonverting to matrix to use with glmnet:
x_train<-as.matrix(x_train)
x_test_scaled<-as.matrix(x_test_scaled)
x_test<-as.matrix(x_test)


x_train_scaled<-as.matrix(x_train_scaled)
#fitting:
fit.lasso_cv <- cv.glmnet(x = x_train_scaled, y = y_train, nfolds = 10)
fit.lasso_cv
```


We can plot the estimated mean squared prediction error $\widehat{MSPE}$ against the tried values of $\lambda$ (in logarithmic scale). Since $\widehat{MSPE}$ is a data-based estimate, we also plot its standard errors. At the top we see the number of coefficients estimated nonzero for the different values of $\lambda$.

```{r}
plot(fit.lasso_cv)
```

We can check how many values of $\lambda$ were assessed, and the value deemed to be optimal according to cross-validation.
```{r}
length(fit.lasso_cv$lambda)
fit.lasso_cv$lambda.min
```
We can also plot the estimated $\hat{\beta}_\lambda$ for all considered $\lambda$.
```{r}
plot(fit.lasso_cv$glmnet.fit, xvar = 'lambda')
```

We retrieve the estimated coefficients $\hat{\beta}_{\hat{\lambda}}$ with the `coef` function. The argument `s='lambda.min'` indicates to set $\hat{\lambda}$ to the value minimizing $\widehat{MSPE}$. By default `coef` uses $\hat{\lambda}_{1SE}= \hat{\lambda} + \mbox{SE} \hat{\lambda}$.
```{r}
b.lasso <- as.vector(coef(fit.lasso_cv, s='lambda.min'))
round(b.lasso, 3)
```
Let's see how many coefficients does lasso cv select.

```{r}
nonzeros <- b.lasso[b.lasso != 0]
nonzeros_sum <- length(nonzeros)
nonzeros
nonzeros_sum
```

### Via BIC and Extended BIC (EBIC)

We repeat the analyses for LASSO setting $\lambda$ via BIC. We use function the`lasso.bic` from `routines_seminar1.R`.

```{r}

lasso.bic <- function(y,x,extended=FALSE) {
  #Select model in LASSO path with best BIC (using LASSO regression estimates)
  #Input
  # - y: vector with response variable
  # - x: design matrix
  #
  #Output: list with the following elements
  # - coef: LASSO-estimated regression coefficient with lambda set via BIC
  # - ypred: predicted y
  # - lambda.opt: optimal value of lambda
  # - lambda: data.frame with bic and number of selected variables for each value of lambda
  require(glmnet)
  fit <- glmnet(x=x,y=y,family='gaussian',alpha=1)
  pred <- cbind(1,x) %*% rbind(fit$a0,fit$beta)
  n <- length(y)
  p <- colSums(fit$beta!=0) + 1
  if (!extended){
    bic <- n * log(colSums((y-pred)^2)/length(y)) + n*(log(2*pi)+1) + log(n)*p 
  } else {
    bic <- n * log(colSums((y-pred)^2)/length(y)) + n*(log(2*pi)+1) + log(n)*p + 2*log(choose(ncol(x),p))
  }
  sel <- which.min(bic)
  beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
  ypred <- pred[,sel]
  ans <- list(coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
  return(ans)
}
```

#### BIC

To use the BIC criteria, we set `extended=FALSE`.
```{r}
fit.lassobic <- lasso.bic(y = y_train, x = x_train_scaled, extended = FALSE)
b.lassobic <- fit.lassobic$coef
names(fit.lassobic)
```

#### EBIC


To use the EBIC criteria, we set `extended=TRUE`.
```{r}
fit.lassoebic <- lasso.bic(y = y_train, x = x_train_scaled, extended = TRUE)
b.lassoebic <- fit.lassoebic$coef
names(fit.lassoebic)

```


### Coefficients

```{r}

b.lassobic <- fit.lassobic$coef
b.lassobic <- as.vector(b.lassobic)
round(b.lassobic, 3)
```


```{r}

b.lassoebic <- fit.lassoebic$coef

b.lassoebic <- as.vector(b.lassoebic)
round(b.lassoebic, 3)
```


Let's summarize the coefficients
```{r}
#nonzeros cv:
nonzeros_cv <- b.lasso[b.lasso != 0]
print(length(nonzeros_cv))

#nonzeros bic#
nonzeros_bic <- b.lassobic[b.lassobic != 0]
print(length(nonzeros_bic))

#nonzeros ebic#
nonzeros_ebic <- b.lassoebic[b.lassoebic != 0]
print(length(nonzeros_ebic))


#check which coefficients we end up using:
b.lasso_no_intercept <- b.lasso[-1]
b.lassobic_no_intercept <- b.lassobic[-1]
b.lassoebic_no_intercept <- b.lassoebic[-1]

coef_lasso_cv <- colnames(x)[b.lasso_no_intercept != 0]

# Find the names of predictors selected by BIC (lasso.bic)
coef_lasso_bic <- colnames(x)[b.lassobic_no_intercept != 0]
# Find the names of predictors selected by BIC (lasso.bic)
coef_lasso_ebic <- colnames(x)[b.lassoebic_no_intercept != 0]
#setdiff(coef_lasso_cv, coef_lasso_bic) # In CV but not in BIC
#setdiff(coef_lasso_bic, coef_lasso_cv) # In BIC but not in CV
```

## Predictiotens

```{r}
y_pred_cv <- predict(fit.lasso_cv, s = "lambda.min", newx = x_test_scaled) #We can use built in predict funciton for lasso_cv

# Add intercept column to the test data
x_test_intercept <- cbind(1, x_test_scaled)

# Predictions using BIC-based LASSO
y_pred_bic <- x_test_intercept %*% fit.lassobic$coef

# Predictions using EBIC-based LASSO
y_pred_ebic <- x_test_intercept %*% fit.lassoebic$coef
```


```{r}
mspe_cv <- mean((y_test - y_pred_cv)^2)
mspe_bic <- mean((y_test - y_pred_bic)^2)
mspe_ebic <- mean((y_test - y_pred_ebic)^2)

r_squared_cv <- 1 - sum((y_test - y_pred_cv)^2) / sum((y_test - mean(y_test))^2)
r_squared_bic <- 1 - sum((y_test - y_pred_bic)^2) / sum((y_test - mean(y_test))^2)
r_squared_ebic <- 1 - sum((y_test - y_pred_ebic)^2) / sum((y_test - mean(y_test))^2)

```


```{r}
cat("MSPE for CV:", mspe_cv, "\n")
cat("MSPE for BIC:", mspe_bic, "\n")
cat("MSPE for EBIC:", mspe_ebic, "\n")


cat("R-squared for CV:", r_squared_cv, "\n")
cat("R-squared for BIC:", r_squared_bic, "\n")
cat("R-squared for EBIC:", r_squared_ebic, "\n")
```
We can thus say that the lambda set via the different methods all deliver similiar results, but each of them includes different coefficients and the best MSPE and R Squarred is archieved with EBIC. It is important to notice that this depends on the test and train split used so further improvement could be made but this was not the goal of the projct.


The chosen coefficients are:
```{r}
print(coef_lasso_cv)

print(coef_lasso_bic)

print(coef_lasso_ebic)
```

## Inference

### LASSO + naive OLS

Here we run a simple but not quite correct approach. we fit an ordinary least squares after doing the lasso and use those estimates as an inference.

In Step 2 we fit a linear model using the variables selected in Step 1 via ordinary least-squares (R function `lm`). We display the first few estimated coefficients, confidence intervals and P-values. 
```{r}
# Selecting the columns from x_train_scaled based on the LASSO coefficients
selected <- as.data.frame(x_train_scaled[, coef_lasso_ebic, drop = FALSE])

# Fit the linear model with the selected variables
lm.afterlasso <- lm(y_train ~ ., data = selected)

# Initialize a matrix to store estimates, confidence intervals, and p-values
ci.lasso <- matrix(NA, nrow = ncol(selected), ncol = 4)
colnames(ci.lasso) <- c('estimate', 'ci_low', 'ci_up', 'p_value')
rownames(ci.lasso) <- paste("Feature", 1:ncol(selected), sep = "")

# Extract the coefficients and confidence intervals for the selected features
ci.lasso[, 1] <- coef(lm.afterlasso)[-1]  # Removing intercept
conf_intervals <- confint(lm.afterlasso)  
ci.lasso[, 2:3] <- conf_intervals[-1, ]   

# Extract p-values for the coefficients
ci.lasso[, 4] <- summary(lm.afterlasso)$coef[-1, 4] 

# Round the estimates and p-values for display
ci.lasso[, 1:3] <- round(ci.lasso[, 1:3], 3)
ci.lasso[, 4] <- round(ci.lasso[, 4], 4)

# Display the results for non-zero coefficients (those selected by LASSO)
ci.lasso[!is.na(ci.lasso[, 1]), ] 

```


We also plot the confidence intervals.
```{r}
# Assuming 'coef(lm.afterlasso)' are the estimated coefficients for each selected feature
estimates <- coef(lm.afterlasso)[-1]  # Removing the intercept

# Plotting the confidence intervals along with the estimates
cols <- ifelse(ci.lasso[, 2] > 0 & ci.lasso[, 3] < 0, 2, 1)  # Color red if CI doesn't include zero

# Set up the plot, defining the limits based on the confidence intervals
plot(
  1:ncol(selected),  # Number of features selected by LASSO
  estimates,  # Use the estimated coefficients as the y-values for the points
  ylim = 1.25 * range(ci.lasso[, 2:3]),  # Set y-limits based on CI ranges (lower and upper bounds)
  xlim = c(0, ncol(selected)),  # Set x-limits based on the number of selected features
  ylab = 'Estimated Coefficient and 95% CI',  # y-axis label
  xlab = 'Feature',  # x-axis label
  main = 'Naive LASSO + OLS: Confidence Intervals',  # Title
  pch = 16,  # Plot solid points for the estimated coefficients
  col = cols  # Color the points based on whether the CI includes zero
)

# Add the confidence intervals as vertical lines (segments)
segments(
  y0 = ci.lasso[, 2],  # Lower bound of CI
  y1 = ci.lasso[, 3],  # Upper bound of CI
  x0 = 1:ncol(selected),  # Position of each feature on the x-axis
  col = cols  # Color the intervals based on whether they include zero
)

# Optionally, add a horizontal line at 0 to see which coefficients have non-zero intervals
abline(h = 0, col = "grey", lty = 2)

```

## Custom Boostrap for proper CI
```{r}
# Step 1: Define the bootstrap function for the coefficients
boot_fn <- function(data, indices) {
  # Bootstrap resampling of data
  data_resampled <- data[indices, ]
  # Refit the linear model on the resampled data
  fit <- lm(y ~ ., data = data_resampled)
  # Extract coefficients
  coef(fit)
}

#Prepare the data for the bootstrap
# Combine the outcome variable (y) with the selected predictors
data_boot <- cbind(y = y_train, x_train_scaled[, coef_lasso_ebic, drop = FALSE])
data_boot <- as.data.frame(data_boot)

#Run the bootstrap
set.seed(123) # For reproducibility
boot_obj <- boot(
  data = data_boot,
  statistic = boot_fn,
  R = 3000 # Number of bootstrap replicates
)

# Step 4: Calculate confidence intervals for each coefficient
p <- ncol(data_boot) - 1 
boot_ci_matrix <- matrix(NA, nrow = p, ncol = 2) # Initialize matrix for CI bounds

for (i in 1:p) {
  ci <- tryCatch(
    boot::boot.ci(boot_obj, type = "perc", index = i + 1), # +1 to skip intercept
    error = function(e) NULL
  )
  
  # Store the percentile CI if available
  if (!is.null(ci) && !is.null(ci$percent)) {
    boot_ci_matrix[i, ] <- ci$percent[4:5] # Extract lower and upper bounds
  }
}

# Determine significance (if CI does not contain zero)
is_significant <- (boot_ci_matrix[, 1] > 0 | boot_ci_matrix[, 2] < 0)

# Add significance status to the results
boot_ci_results <- data.frame(
  coefficient = colnames(data_boot)[-1], # Exclude intercept
  ci_low = boot_ci_matrix[, 1],
  ci_up = boot_ci_matrix[, 2],
  significant = is_significant
)

# Step 7: Display results
print("Bootstrap confidence intervals and significance for LASSO-selected coefficients:")
print(boot_ci_results)

# Step 8: Plot results, marking significant coefficients
coef_values <- coef(lm(y ~ ., data = data_boot))[-1] # Exclude intercept
plot(
  1:p,
  coef_values,
  ylim = range(boot_ci_matrix, na.rm = TRUE),
  pch = 16, xlab = "Coefficient Index", ylab = "Coefficient Value",
  main = "Bootstrap Confidence Intervals for LASSO Coefficients"
)
segments(
  x0 = 1:p, x1 = 1:p,
  y0 = boot_ci_matrix[, 1], y1 = boot_ci_matrix[, 2],
  col = ifelse(is_significant, "blue", "gray") # Blue for significant, gray otherwise
)
points(1:p, coef_values, pch = 16, col = ifelse(is_significant, "blue", "gray"))
abline(h = 0, lty = 2, col = "red")

legend(
  "topright",
  legend = c("Significant", "Not Significant"),
  col = c("blue", "gray"),
  pch = 16,
  bty = "n"
)

```

# Bootstrapping with bootLasso

Here we apply Lasso to all bootstrapped samples and not only bootstrapp sample after already doing lasso!
Unfortunately, here we cannot set lambda manually as we intend to in the project. We nevertheless include this here as this is the traditional way of seeing if those variables ares significant.

```{r}
# Run bootstrap LASSO
bootlasso <- bootLasso(x_train_scaled, y_train, B = 400, type.boot = "paired", alpha = 0.05)

# Extract confidence intervals
ci.bootlasso <- t(bootlasso$interval)

# Create a dataframe with proper feature names
df <- data.frame(
  variable = colnames(x_train_scaled),  # Use the column names from x_train_scaled
  estimate = bootlasso$Beta,           # LASSO coefficients
  ci.low = ci.bootlasso[, 1],          # Lower confidence interval
  ci.up = ci.bootlasso[, 2]            # Upper confidence interval
)

# Filter to show only non-zero estimates
df_filtered <- df[df$estimate != 0, ]

# Display the filtered dataframe
df_filtered

```


# Ridge

## Setting penalization parameter $\lambda$

To set $\lambda$ by cross-validation we use the function `cv.glmnet` again but this time setting parameter `alpha` to 0.

```{r}
fit.ridge <- cv.glmnet(x = x_train_scaled, y = y_train, alpha = 0, nfolds = 10)
fit.ridge
```

We plot the estimated mean squared prediction error $\widehat{MSPE}$ as a function of $\lambda$.

```{r}
plot(fit.ridge)
```

We plot the cofficient path and see that, this time, all estimated coefficients shrink as $\lambda$ grows but remain non-zero.   

```{r}
plot(fit.ridge$glmnet.fit, xvar='lambda')
```

```{r}
b.ridge <- as.vector(coef(fit.ridge, s = 'lambda.min'))
round(b.ridge, 3)
```
Compare it to MLE coef
```{r}
b.mle <- as.vector(coef(fit.mle))
print(b.mle, round = 3)
```






# Bayesian Gaussian regression 

To fit a Gaussian linear regression you set `family` to `gaussian()` in `stan_glm`.

## Ridge like:
```{r}
fit.l2.bayes <- stan_glm(covid_19_scaled$total_cases.y ~ ., data = covid_19_scaled, family = gaussian(), algorithm='sampling')
b.l2.bayes <- coef(fit.l2.bayes)
```
## lasso penalty

We can change the prior
```{r}
fit.l1.bayes <- stan_glm(covid_19_scaled$total_cases.y ~ ., 
                            data = covid_19_scaled, 
                            family = gaussian(), 
                            algorithm = 'sampling',
                            prior = lasso(1))  # Laplace prior with scale of 1
b.l1.bayes <- coef(fit.l1.bayes)

```



```{r}
print(b.l2.bayes)
```


```{r}
print(b.mle)
print(length(b.mle))
```


Plot the different coefficients:
```{r}
data.frame(mle= b.mle, bayes= b.l2.bayes) %>% 
  ggplot(aes(x=mle,y=bayes)) + 
  geom_point(shape = "O",size=2) +
  geom_abline(slope=1, intercept = 0, linetype = 'dashed') +
  geom_hline(yintercept = 0, linetype = 'dotted') +
  xlab('MLE OLS') +
  ylab('MCMC Bayesian regression') +
  coord_cartesian(xlim=c(-2*10^6,2*10^6),ylim=c(-2*10^6,2*10^6)) +
  theme_classic()
```

### Bayesian model selection

We also include some initial appraoches of how we would tackle our research question in a more Bayesian way. 
As described in the report, lasso penalty worked the best so we decided to mainly describe that approach.


## Fitting

We run the Bayesian model selection implemented in the function `modelSelection` from R package `mombf`. The option `enumerate`, if set to `TRUE`, forces full model enumeration, but more generally when the number of parameters is large it’s better to use the default, which uses Gibbs sampling to search the model space.

We set a Beta-Binomial(1,1) prior on the models and Zellner’s prior on the regression coefficients. The prior dispersion parameter `taustd` corresponds to $g$ in our notation, for which we use the default $g=1$.

```{r}
fit.bayesreg <- modelSelection(
    y=y_train,
    x=as.matrix(x_train_scaled), 
    priorCoef=zellnerprior(taustd=1), 
    priorDelta=modelbbprior(1,1)
)
```

Note that the Gaussian shrinkage prior is also available in `mombf` as `normalidprior()`, with parameter `taustd` corresponding to $g$ in our notation.

```{r}
head(postProb(fit.bayesreg), 10)
```
## Model averaging estimates of coefficients 

With the method `coef` we extract Bayesian model averaging estimates for each coefficient, posterior intervals and posterior marginal inclusion probabilities ($P(\beta_j \neq 0 | \mathbf{y}) = P(\gamma_j = 1 | \mathbf{y})$). 

```{r}
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3] <- round(ci.bayesreg[,1:3], 3)  
ci.bayesreg[,4] <- round(ci.bayesreg[,4], 4)      
head(ci.bayesreg)
tail(ci.bayesreg)
```





# Part 2 of the course:

```{r}

# Creating a new variable as a copy of covid_19_scaled
covid_19_new <- covid_19_scaled

# Rename the column "total_cases.y" to "total_cases" in the new variable
names(covid_19_new)[names(covid_19_new) == "total_cases.y"] <- "total_cases"
covid_19 <- covid_19_new

```

-------Implement K-means--------
## Kmeans
```{r}
# Check for NA values in each column
na_summary <- colSums(is.na(covid_19))

# Display columns with NA values
na_columns <- na_summary[na_summary > 0]
print(na_columns)
```
```{r}
# Check which columns are not numeric
non_numeric_cols <- covid_19 %>%
  select(where(~ !is.numeric(.)))
print(colnames(non_numeric_cols))
```

```{r}

# Preparing the Data
df <- covid_19 %>% select(-total_cases)

# Compute Gap Statistic using Default K-means
set.seed(123)  # For reproducibility
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25, K.max = 10, B = 25)
# K.max = maximum number of clusters to test
# B = number of bootstrap samples

# Display Optimal Number of Clusters
cat("Optimal number of clusters based on Gap Statistic:\n")
print(gap_stat, method = "firstmax")

# Visualize the Gap Statistic
fviz_gap_stat(gap_stat) +
  labs(title = "Gap Statistic for Optimal Number of Clusters")

```
As we can see the GAP Statistic is giving us an optimal of 3 clusters, so now we wanted to compare it with the Elbow method and see how our clusters look like.
```{r}
# Prepare Data for Clustering
# Exclude the target variable `total_cases`
df_kmeans <- covid_19 %>% select(-total_cases)

# Determine Optimal Number of Clusters
# Elbow Method
elbow_plot <- fviz_nbclust(df_kmeans, kmeans, method = "wss") +
  labs(title = "Elbow Method for Optimal Clusters")

# Gap Statistic
set.seed(123)  # For reproducibility
gap_stat <- clusGap(df_kmeans, FUN = kmeans, nstart = 25, K.max = 10, B = 25)

# Visualize the Gap Statistic
gap_stat_plot <- fviz_gap_stat(gap_stat) +
  labs(title = "Gap Statistic for Optimal Clusters")

# Combine both plots side-by-side using patchwork
combined_plot <- elbow_plot / gap_stat_plot

# Display the combined plot
print(combined_plot)

# Step 3: Apply K-means
# Set number of clusters to k = 3 (determined from Gap Statistic)
set.seed(123)  # For reproducibility
kmeans_result <- kmeans(df_kmeans, centers = 3, nstart = 25)

# Add cluster assignments to the original data
df_with_clusters <- covid_19 %>%
  mutate(cluster = kmeans_result$cluster)

# Step 4: Visualize Clusters
# Visualize in 2D using PCA
fviz_cluster(kmeans_result, data = df_kmeans,
             geom = "point", ellipse.type = "convex") +
  labs(title = "K-means Clustering for k = 3 Clusters")

# Step 5: Summarize Results
# View cluster means for interpretation
cluster_means <- df_with_clusters %>%
  group_by(cluster) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

print(cluster_means)  # Inspect cluster averages

```
It is evident that the Elbow method would also suggest 3 clusters.
While checking our cluster outcome we saw that there was a potential outlier in our data, however when checking it closely we saw that that is an actual observation, we decided to keep it and see the outcome in the next results.

###Identifying Outliers
```{r}


# Step 1: Perform PCA on the scaled K-means input data
pca_result <- prcomp(df_kmeans, scale. = TRUE)

# Extract PCA scores for the first two components
pca_scores <- as.data.frame(pca_result$x)

# Step 2: Add cluster assignments and observation IDs to the PCA scores
pca_scores_with_clusters <- pca_scores %>%
  mutate(cluster = kmeans_result$cluster, 
         observation_id = rownames(df_kmeans))  # Add observation IDs for tracking

# Step 3: Identify Potential Outliers Based on PCA Scores
# Adjust the threshold (e.g., PC1 > 15, PC2 < -30) based on your scatterplot
outliers <- pca_scores_with_clusters %>%
  filter(PC2 > 30 | PC2 < -30)  # Modify thresholds based on observed graph

# Print the details of the outliers
print("Outliers identified in PCA:")
print(outliers)

# Step 4: Trace the Outlier(s) Back to the Original Dataset
outlier_details <- covid_19 %>%
  mutate(observation_id = rownames(covid_19)) %>%
  filter(observation_id %in% outliers$observation_id)

# Print detailed information about the outlier(s) in the original dataset
print("Details of the outlier(s) in the original dataset:")
print(outlier_details)

# Step 5: Map Outlier to Specific Country (if applicable)
# Ensure `covid_19_countries` has proper observation IDs
names(covid_19_countries) <- make.names(names(covid_19_countries), unique = TRUE)

covid_19_countries <- covid_19_countries %>%
  mutate(observation_id = rownames(covid_19_countries))

outlier_with_country <- covid_19_countries %>%
  filter(observation_id %in% outliers$observation_id)

# Display the country name and details for the outlier(s)
print("Outlier details with country name:")
print(outlier_with_country)

# Step 6: Visualize the Outliers on PCA Plot
ggplot(pca_scores_with_clusters, aes(x = PC1, y = PC2, color = as.factor(cluster))) +
  geom_point() +
  geom_point(data = outliers, aes(x = PC1, y = PC2), color = "black", size = 3, shape = 8) +
  labs(title = "PCA Plot Highlighting Outliers", x = "PC1", y = "PC2", color = "Cluster") +
  theme_minimal()

```

###Analyzing Clusterin K-Means
```{r}
cluster_sizes <- df_with_clusters %>%
  group_by(cluster) %>%
  summarise(Count = n())

print(cluster_sizes)

```
This is the size of our clusters.

```{r}
key_features_summary <- df_with_clusters %>%
  group_by(cluster) %>%
  summarise(
    avg_gdp_per_capita = mean(gdp_per_capita, na.rm = TRUE),
    avg_life_expectancy = mean(life_expectancy, na.rm = TRUE),
    avg_healthcare_infra = mean(hospital_beds_per_thousand, na.rm = TRUE)
  )

print(key_features_summary)

```
And here we decided to analyze some variables to see how the countries selected relate to each other. With this analysis we could say:
Cluster 1 includes countries with a moderate average GDP per capita (-0.07), slightly above-average life expectancy (0.18), and below-average healthcare infrastructure (-0.19), suggesting countries in a transitional stage of development. In contrast, Cluster 2 represents countries with the highest average GDP per capita (0.94), life expectancy (1.01), and healthcare infrastructure (1.01), reflecting well-developed economies and robust healthcare systems. Finally, Cluster 3 contains countries with the lowest average GDP per capita (-0.75), life expectancy (-1.22), and healthcare infrastructure (-0.61), indicating significant economic and healthcare challenges.

Note: The reported values are standardized (z-scores) to ensure comparability across variables, as the data was centered and scaled before clustering. These values represent deviations from the mean, measured in standard deviations.

## Applying PCA
Given the limitations of K-means clustering in high-dimensional data, particularly the potential for redundancy and noise among variables, we next explore Gaussian Mixture Models (GMM). To further improve the clustering results and address the curse of dimensionality, we integrate Principal Component Analysis (PCA) for dimensionality reduction before applying GMM. This approach enables us to retain the most significant patterns in the data while reducing its complexity, thereby enhancing the interpretability and robustness of the clustering process.

```{r}
# Exclude the target variable (total_cases)
df_pca <- covid_19 %>% select(-total_cases)

# Standardize the data (z-score normalization)
df_pca <- scale(df_pca)

```

```{r}
# Run PCA
pca_result <- prcomp(df_pca, center = TRUE, scale. = TRUE)

# Summary of PCA
summary(pca_result)

```

```{r}
# Plot the proportion of variance explained
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50)) +
  labs(title = "Scree Plot: Variance Explained by Principal Components")

```
PC1 explains 31.2% of the variance – this is a significant proportion.
The variance explained drops sharply after PC1, with PC2 (11%) and PC3 (10.4%).
Beyond PC4 (~9.5%), the contribution of additional PCs diminishes gradually.
The "elbow" (inflection point) in the scree plot occurs at PC4-5, where the curve starts leveling off.

This is why we will retain the first 5 components
These PCs explain a combined ~70% of the variance.
This choice balances dimensionality reduction while retaining significant information.

```{r}
# Extract the first 5 PCs
pca_scores <- as.data.frame(pca_result$x[, 1:5])

# Add the target variable back for analysis
pca_scores$total_cases <- covid_19$total_cases

# Check the transformed data
head(pca_scores)

```
The 5 PCs are linear combinations of the original features.
These PCs represent the most important directions of variation in the data, with PC1 explaining the largest variance (~31.2%) and subsequent PCs capturing smaller amounts of variance.
Each principal component (PC) provides insight into how different observations relate to one another in the reduced space.


We can also see how to interpret these PCs, at taking a close look at the loadings:
```{r}
# Extract the PCA loadings (coefficients for each component)
loadings <- pca_result$rotation

# View the loadings for the first few components
head(loadings)

```

This can be done for each Principal component, we include an example of the first one here.
The output you provides the loadings of various features or interactions between features on the first principal component (PC1) of the PCA analysis. These loadings represent the contribution of each feature or interaction term to the principal component. 
```{r}
# Extract the loadings for the first principal component (PC1)
component <- 2 #change this to analyze for different components
loadings_PC1 <- loadings[, component] 

# Sort the loadings by absolute value in decreasing order
loadings_PC1_sorted <- sort(abs(loadings_PC1), decreasing = TRUE)

# Convert the sorted loadings to percentages
loadings_PC1_percent <- loadings_PC1_sorted * 100

# Print the sorted loadings and their corresponding percentages
loadings_PC1_percent
```


### Correlation Analysis
```{r}
# Calculate correlations between PCs and total_cases
correlations <- cor(pca_scores[, 1:5], pca_scores$total_cases)
print(correlations)

```

To explore the relationship between the principal components and total_cases, we conducted a correlation analysis. The results show that PC4 has the strongest negative correlation with total_cases at -0.662, indicating that lower PC4 values align with higher total case counts. In contrast, PC1 exhibits the strongest positive correlation at 0.296, suggesting that higher values of PC1 are moderately associated with an increase in case counts. PC3 also displays a weaker positive correlation of 0.239, while PC2 and PC5 exhibit weak negative correlations of -0.236 and -0.048, respectively.

### Plotting PC1 and PC4
```{r}
# Custom theme for cleaner visuals
custom_theme <- theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    axis.title = element_text(face = "bold", size = 14),
    axis.text = element_text(size = 12)
  )

# Scatterplot for PC1 vs Total Cases with log-transformed y-axis
plot_pc1 <- ggplot(pca_scores, aes(x = PC1, y = total_cases)) +
  geom_point(color = "lightblue", alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  scale_y_log10() +
  custom_theme +
  labs(title = "PC1 vs Total Cases",
       x = "PC1",
       y = "Total Cases")

# Scatterplot for PC4 vs Total Cases with log-transformed y-axis
plot_pc4 <- ggplot(pca_scores, aes(x = PC4, y = total_cases)) +
  geom_point(color = "lightblue", alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  scale_y_log10() +
  custom_theme +
  labs(title = "PC4 vs Total Cases",
       x = "PC4",
       y = "Total Cases")

# Arrange the two plots in a single grid
grid.arrange(plot_pc1, plot_pc4, ncol = 2)

```
We are specifically analyzing PC1 and PC4 because they show the strongest correlations with total cases: PC1 has a positive correlation reflecting key variables that contribute to case increases, while PC4 has a negative correlation, highlighting factors that may suppress or counteract higher case counts. This analysis validates the importance of PC1 and PC4 in explaining variability and correlations, providing a foundation for our next step which is clustering using Gaussian Mixture Models (GMM).

Importance of PCA
Principal Component Analysis (PCA) reduces high-dimensional data into fewer components while retaining key patterns.


## GMMS
Overall, PCA successfully reduced the dataset's complexity, identifying PCs that highlight latent relationships with the target variable. The correlations of PC1 and PC4 with demonstrate the method's ability to uncover underlying structure in the data while reducing redundancy. We next employ Gaussian Mixture Models (GMM). By applying GMM to the PCA-transformed dataset, we aim to achieve more flexible and interpretable clusters while leveraging the reduced dimensional space to enhance computational efficiency and cluster quality.
```{r}

# Prepare PCA-transformed Data
# Retain the first 5 principal components
pca_data <- as.data.frame(pca_result$x[, 1:5])  
colnames(pca_data) <- c("PC1", "PC2", "PC3", "PC4", "PC5")

# Apply Gaussian Mixture Models
set.seed(123)  # For reproducibility
gmm_result <- Mclust(pca_data)

# Summary of GMM Results
summary(gmm_result)  

# Visualize GMM BIC Values
plot(gmm_result, what = "BIC")  # Plot BIC values to validate the optimal number of clusters

# Add GMM Cluster Assignments to PCA Data
pca_data <- pca_data %>%
  mutate(gmm_cluster = gmm_result$classification)  # Explicitly name the new cluster column

# Visualize Clusters in PCA Space
# Scatter plot for the first two principal components
ggplot(pca_data, aes(x = PC1, y = PC2, color = factor(gmm_cluster))) +
  geom_point(size = 2, alpha = 0.6) +
  labs(title = "GMM Clustering in PCA-Reduced Space",
       x = "Principal Component 1",
       y = "Principal Component 2",
       color = "Cluster") +
  theme_minimal()

# Analyze Cluster Means
# Summarize the cluster means for the principal components
cluster_summary <- pca_data %>%
  group_by(gmm_cluster) %>%
  summarise(across(starts_with("PC"), mean, na.rm = TRUE))

# Display the cluster summary
print(cluster_summary)

```
The Bayesian Information Criterion (BIC) identified the optimal clustering solution as the VEV model, characterized by ellipsoidal clusters with equal shape. This model achieves a BIC value of -4224.35, balancing model complexity and fit. The resulting Gaussian Mixture Model (GMM) solution produced three clusters of sizes 88, 12, and 75, as summarized in. These clusters were further visualized in the PCA-reduced space, where clear separations across the first two principal components were observed, illustrating the structure captured by the GMM. 


## Country Assignment
```{r}
# Prepare PCA-transformed Data
pca_data <- as.data.frame(pca_result$x[, 1:5])  
colnames(pca_data) <- c("PC1", "PC2", "PC3", "PC4", "PC5")  

# Apply Gaussian Mixture Models
set.seed(123)  # For reproducibility
gmm_result <- Mclust(pca_data)

# Add GMM Cluster Assignments to PCA Data
pca_data$gmm_cluster <- gmm_result$classification

# Re-add the 'location' column
# Merge the cluster assignments with the original saved data
covid_19_countries <- covid_19_countries %>% 
  mutate(gmm_cluster = pca_data$gmm_cluster)  # Add cluster info

# Display the results
# View the country names and their corresponding clusters
country_clusters <- covid_19_countries %>% 
  select(location, gmm_cluster)

print(country_clusters)  # Display the table with country names and clusters

```
Here we can see the cluster assignment but to see it clearler we decided to plot a map.
```{r}
# Step 1: Merge total_cases into covid_19_countries
covid_19_countries <- covid_19_countries %>%
  mutate(total_cases = covid_19$total_cases)

# Step 2: Summarize by GMM clusters
covid_19_countries %>%
  group_by(gmm_cluster) %>%
  summarise(
    mean_cases = mean(total_cases, na.rm = TRUE)
  )


```
## Plotting World Map

This code is commented out as it is just plotting the results seen in the report and requires some heavy dependenes.
It can always be commented out and should run without any problems then.
```{r}
#install.packages("ggplot2")
#install.packages("rnaturalearth")
#install.packages("rnaturalearthdata")
#install.packages("ggthemes")

```

```{r}
library(ggplot2)
library(dplyr)
library(rnaturalearth)   # To get country map data
library(rnaturalearthdata)
library(ggthemes)        # For clean map themes

# Step 1: Prepare the GMM cluster data with country names
# Assuming 'covid_19_countries' contains the "location" column and GMM clusters
cluster_map_data <- covid_19_countries %>%
  select(location, gmm_cluster) %>%
  distinct(location, gmm_cluster)  # Keep only unique country-cluster pairs

# Step 2: Fix the "United States" naming issue
cluster_map_data <- cluster_map_data %>%
  mutate(location = recode(location, "United States" = "United States of America"))

# Step 3: Load world map data
world <- ne_countries(scale = "medium", returnclass = "sf")  # Natural Earth map

# Step 4: Merge the cluster data with the world map
world_clusters <- world %>%
  left_join(cluster_map_data, by = c("name" = "location"))  # Match by country names

# Step 5: Plot the world map with clusters
ggplot(data = world_clusters) +
  geom_sf(aes(fill = factor(gmm_cluster)), color = "white", size = 0.2) +
  scale_fill_manual(
    values = c("#1b9e77", "#d95f02", "#7570b3"),  # Customize cluster colors
    name = "Cluster", 
    labels = c("1", "2", "3")  # Rename clusters if needed
  ) +
  labs(
    title = "World Map of GMM Clusters",
    subtitle = "Countries grouped based on demographic, economic, and healthcare variables",
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

```
The clustering results reveal fascinating insights, particularly the placement of the United States, China, and India in Cluster 2. These three countries recorded the highest total COVID-19 case counts, indicating that the GMM approach, combined with PCA, successfully identified critical underlying patterns in the data. This demonstrates the model's ability to capture essential relationships among demographic, economic, and healthcare variables, which led to the grouping of countries with vastly different socioeconomic contexts but similar pandemic outcomes. This outcome underscores the effectiveness of GMM in uncovering latent structures that align with real-world phenomena, even when total cases were not directly included in the clustering process.



